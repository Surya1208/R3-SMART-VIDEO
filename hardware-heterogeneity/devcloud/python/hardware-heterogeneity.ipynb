{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Intel® Distribution of OpenVINO™ toolkit hetero plugin\n",
    "\n",
    "\n",
    "    \n",
    "This example shows how to use hetero plugin to define preferences to run different network layers on different hardware types. Here, we will use the command line option to define hetero plugin usage where the layer distribution is already defined. However, hetero plugin also allows developers to customize distribution of layers execution on different hardware by specifying it in the application code.\n",
    "\n",
    "## Car detection tutorial example\n",
    "\n",
    "### 1. Importing dependencies, Setting the Environment variables and Generate the IR files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "import time\n",
    "import sys                                     \n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent.parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[setupvars.sh] OpenVINO environment initialized\r\n"
     ]
    }
   ],
   "source": [
    "!/opt/intel/openvino/bin/setupvars.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading models ||################\n",
      "\n",
      "========== Downloading models/public/mobilenet-ssd/mobilenet-ssd.prototxt\n",
      "... 100%, 28 KB, 52579 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading models/public/mobilenet-ssd/mobilenet-ssd.caffemodel\n",
      "... 100%, 22605 KB, 22600 KB/s, 1 seconds passed\n",
      "\n",
      "################|| Post-processing ||################\n",
      "\n",
      "################|| Downloading models ||################\n",
      "\n",
      "========== Downloading models/public/squeezenet1.1/squeezenet1.1.prototxt\n",
      "... 100%, 9 KB, 26674 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading models/public/squeezenet1.1/squeezenet1.1.caffemodel\n",
      "... 100%, 4834 KB, 27628 KB/s, 0 seconds passed\n",
      "\n",
      "################|| Post-processing ||################\n",
      "\n",
      "========== Replacing text in models/public/squeezenet1.1/squeezenet1.1.prototxt\n"
     ]
    }
   ],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name mobilenet-ssd  -o models\n",
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name squeezenet1.1  -o models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\" # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"6\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"6\" # export NUMEXPR_NUM_THREADS=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/public/mobilenet-ssd/mobilenet-ssd.caffemodel\n",
      "\t- Path for generated IR: \t/home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/mobilenet-ssd/FP32/\n",
      "\t- IR output name: \tmobilenet-ssd\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \t[127,127,127]\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \t256.0\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Path to Python Caffe* parser generated from caffe.proto: \t/opt/intel/openvino/deployment_tools/model_optimizer/mo/front/caffe/proto\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/public/mobilenet-ssd/mobilenet-ssd.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \tDefault\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "Model Optimizer version: \t2019.3.0-375-g332562022\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: /home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/mobilenet-ssd/FP32/mobilenet-ssd.xml\n",
      "[ SUCCESS ] BIN file: /home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/mobilenet-ssd/FP32/mobilenet-ssd.bin\n",
      "[ SUCCESS ] Total execution time: 8.15 seconds. \n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/public/squeezenet1.1/squeezenet1.1.caffemodel\n",
      "\t- Path for generated IR: \t/home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/squeezenet1.1/\n",
      "\t- IR output name: \tsqueezenet1.1\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \t[127,127,127]\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \t256.0\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Path to Python Caffe* parser generated from caffe.proto: \t/opt/intel/openvino/deployment_tools/model_optimizer/mo/front/caffe/proto\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/public/squeezenet1.1/squeezenet1.1.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \tDefault\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "Model Optimizer version: \t2019.3.0-375-g332562022\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: /home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/squeezenet1.1/squeezenet1.1.xml\n",
      "[ SUCCESS ] BIN file: /home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/squeezenet1.1/squeezenet1.1.bin\n",
      "[ SUCCESS ] Total execution time: 3.86 seconds. \n"
     ]
    }
   ],
   "source": [
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model models/public/mobilenet-ssd/mobilenet-ssd.caffemodel -o models/mobilenet-ssd/FP32/ --scale 256 --mean_values [127,127,127]\n",
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model models/public/squeezenet1.1/squeezenet1.1.caffemodel -o models/squeezenet1.1/  --scale 256 --mean_values [127,127,127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2. Run the car detection tutorial with hetero plugin\n",
    "\n",
    "\n",
    "#### Create Job Script \n",
    "\n",
    "We will run the workload on several DevCloud's edge compute nodes. We will send work to the edge compute nodes by submitting jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "To pass the specific variables to the Python code, we will use following arguments:\n",
    "\n",
    "* `-f`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;location of the optimized models XML\n",
    "* `-i`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;location of the input video\n",
    "* `-r`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output directory\n",
    "* `-d`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hardware device type (CPU, GPU, MYRIAD, HDDL or HETERO:FPGA,CPU)\n",
    "* `-n`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;number of infer requests\n",
    "\n",
    "The job file will be executed directly on the edge compute node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting object_detection.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile object_detection.sh\n",
    "\n",
    "ME=`basename $0`\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# Object detection script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "# The output directory is the first argument of the bash script\n",
    "while getopts 'd:f:i:r:n:?' OPTION; do\n",
    "    case \"$OPTION\" in\n",
    "    d)\n",
    "        DEVICE=$OPTARG\n",
    "        echo \"$ME is using device $OPTARG\"\n",
    "      ;;\n",
    "\n",
    "    f)\n",
    "        FP_MODEL=$OPTARG\n",
    "        echo \"$ME is using floating point model $OPTARG\"\n",
    "      ;;\n",
    "\n",
    "    i)\n",
    "        INPUT_FILE=$OPTARG\n",
    "        echo \"$ME is using input file $OPTARG\"\n",
    "      ;;\n",
    "    r)\n",
    "        RESULTS_BASE=$OPTARG\n",
    "        echo \"$ME is using results base $OPTARG\"\n",
    "      ;;\n",
    "    n)\n",
    "        NUM_INFER_REQS=$OPTARG\n",
    "        echo \"$ME is running $OPTARG inference requests\"\n",
    "      ;;\n",
    "    esac  \n",
    "done\n",
    "\n",
    "NN_MODEL=\"mobilenet-ssd.xml\"\n",
    "RESULTS_PATH=\"${RESULTS_BASE}\"\n",
    "mkdir -p $RESULTS_PATH\n",
    "echo \"$ME is using results path $RESULTS_PATH\"\n",
    "\n",
    "if [ \"$DEVICE\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/opt/altera/aocl-pro-rte/aclrte-linux64/\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    source /opt/fpga_support_files/setup_env.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_bitstreams/2019R1_PL1_FP11_MobileNet_Clamp.aocx\n",
    "fi\n",
    "    \n",
    "# Running the object detection code\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "python3 tutorial1.py                        -m models/mobilenet-ssd/${FP_MODEL}/${NN_MODEL}  \\\n",
    "                                            -i $INPUT_FILE \\\n",
    "                                            -o $RESULTS_PATH \\\n",
    "                                            -d $DEVICE \\\n",
    "                                            -nireq $NUM_INFER_REQS \\\n",
    "                                            -ce /opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so\n",
    "\n",
    "g++ -std=c++14 ROI_writer.cpp -o ROI_writer  -lopencv_core -lopencv_videoio -lopencv_imgproc -lopencv_highgui  -fopenmp -I/opt/intel/openvino/opencv/include/ -L/opt/intel/openvino/opencv/lib/\n",
    "# Rendering the output video\n",
    "SKIPFRAME=1\n",
    "RESOLUTION=0.5\n",
    "./ROI_writer $INPUT_FILE $RESULTS_PATH $SKIPFRAME $RESOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Prioritizing running on GPU first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VIDEO\"] = \"cars_1900.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2915.v-qsvr-1.devcloud-edge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8d98ffdf4f406da17c85ef2b7863e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Preprocessing', style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9dc80ec58740cb8b56951a867a8c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Inference', style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce235e05e3674d6ea3ac109fdde50dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Rendering', style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "########################################################################\r\n",
      "#      Date:           Thu Nov 14 01:22:01 PST 2019\r\n",
      "#    Job ID:           2915.v-qsvr-1.devcloud-edge\r\n",
      "#      User:           u33131\r\n",
      "# Resources:           neednodes=1:idc001skl:intel-hd-530,nodes=1:idc001skl:intel-hd-530,walltime=01:00:00\r\n",
      "########################################################################\r\n",
      "\r\n",
      "[setupvars.sh] OpenVINO environment initialized\r\n",
      "2915.v-qsvr-1.devcloud-edge.SC is using results base results/GPU\r\n",
      "2915.v-qsvr-1.devcloud-edge.SC is using device HETERO:GPU,CPU\r\n",
      "2915.v-qsvr-1.devcloud-edge.SC is using floating point model FP32\r\n",
      "2915.v-qsvr-1.devcloud-edge.SC is using input file cars_1900.mp4\r\n",
      "2915.v-qsvr-1.devcloud-edge.SC is running 4 inference requests\r\n",
      "2915.v-qsvr-1.devcloud-edge.SC is using results path results/GPU\r\n",
      "[ INFO ] Initializing plugin for HETERO:GPU,CPU device...\r\n",
      "[ INFO ] Loading plugins for HETERO:GPU,CPU device...\r\n",
      "[ INFO ] Reading IR...\r\n",
      "[ INFO ] Loading IR to the plugin...\r\n",
      "mbox_conf_reshape is GPU\r\n",
      "conv11_mbox_conf_perm is GPU\r\n",
      "conv11_mbox_loc_perm is GPU\r\n",
      "conv13_mbox_conf_perm is GPU\r\n",
      "conv13_mbox_loc_perm is GPU\r\n",
      "conv14_2_mbox_conf_perm is GPU\r\n",
      "conv14_2_mbox_loc_perm is GPU\r\n",
      "conv15_2_mbox_conf_perm is GPU\r\n",
      "conv15_2_mbox_loc_perm is GPU\r\n",
      "conv16_2_mbox_conf_perm is GPU\r\n",
      "conv16_2_mbox_loc_perm is GPU\r\n",
      "conv17_2_mbox_conf_perm is GPU\r\n",
      "conv17_2_mbox_loc_perm is GPU\r\n",
      "[ INFO ] Starting inference in async mode, 4 requests in parallel...\r\n",
      "hello\r\n",
      "hello 2\r\n",
      "Hello 3\r\n",
      "Hello 4\r\n",
      "[ INFO ] Processing done...\r\n",
      "Video post-processing time: 39.2289 seconds\r\n",
      "\r\n",
      "########################################################################\r\n",
      "# End of output for job 2915.v-qsvr-1.devcloud-edge\r\n",
      "# Date: Thu Nov 14 01:24:10 PST 2019\r\n",
      "########################################################################\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub object_detection.sh -l nodes=1:idc001skl:intel-hd-530 -F \"-r results/GPU -d HETERO:GPU,CPU -f FP32 -i $VIDEO -n 4\" -N obj_det_gpu \n",
    "print(job_id_gpu[0]) \n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/GPU', 'pre_progress.txt', \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/GPU', 'i_progress.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/GPU', 'post_progress.txt', \"Rendering\", 0, 100)\n",
    "    \n",
    "while True:\n",
    "    var=job_id_gpu[0].split(\".\")\n",
    "    file=\"obj_det_gpu.o\"+var[0]\n",
    "    if os.path.isfile(file): \n",
    "        ! cat $file\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "#### b) Prioritizing running on CPU first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2917.v-qsvr-1.devcloud-edge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584dd96edaed4df0a7cb8076644c2546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Preprocessing', style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3799d153c63d4b03843fbb8ff125ab73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Inference', style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e596294f49463d92a82a70762faac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Rendering', style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "########################################################################\r\n",
      "#      Date:           Thu Nov 14 01:24:24 PST 2019\r\n",
      "#    Job ID:           2917.v-qsvr-1.devcloud-edge\r\n",
      "#      User:           u33131\r\n",
      "# Resources:           neednodes=1:idc001skl:tank-870:i5-6500te,nodes=1:idc001skl:tank-870:i5-6500te,walltime=01:00:00\r\n",
      "########################################################################\r\n",
      "\r\n",
      "[setupvars.sh] OpenVINO environment initialized\r\n",
      "2917.v-qsvr-1.devcloud-edge.SC is using results base results/Core\r\n",
      "2917.v-qsvr-1.devcloud-edge.SC is using device HETERO:CPU,GPU\r\n",
      "2917.v-qsvr-1.devcloud-edge.SC is using floating point model FP32\r\n",
      "2917.v-qsvr-1.devcloud-edge.SC is using input file cars_1900.mp4\r\n",
      "2917.v-qsvr-1.devcloud-edge.SC is running 4 inference requests\r\n",
      "2917.v-qsvr-1.devcloud-edge.SC is using results path results/Core\r\n",
      "[ INFO ] Initializing plugin for HETERO:CPU,GPU device...\r\n",
      "[ INFO ] Loading plugins for HETERO:CPU,GPU device...\r\n",
      "[ INFO ] Reading IR...\r\n",
      "[ INFO ] Loading IR to the plugin...\r\n",
      "mbox_conf_reshape is GPU\r\n",
      "conv11_mbox_conf_perm is GPU\r\n",
      "conv11_mbox_loc_perm is GPU\r\n",
      "conv13_mbox_conf_perm is GPU\r\n",
      "conv13_mbox_loc_perm is GPU\r\n",
      "conv14_2_mbox_conf_perm is GPU\r\n",
      "conv14_2_mbox_loc_perm is GPU\r\n",
      "conv15_2_mbox_conf_perm is GPU\r\n",
      "conv15_2_mbox_loc_perm is GPU\r\n",
      "conv16_2_mbox_conf_perm is GPU\r\n",
      "conv16_2_mbox_loc_perm is GPU\r\n",
      "conv17_2_mbox_conf_perm is GPU\r\n",
      "conv17_2_mbox_loc_perm is GPU\r\n",
      "[ INFO ] Starting inference in async mode, 4 requests in parallel...\r\n",
      "hello\r\n",
      "hello 2\r\n",
      "Hello 3\r\n",
      "Hello 4\r\n",
      "[ INFO ] Processing done...\r\n",
      "Video post-processing time: 35.9497 seconds\r\n",
      "\r\n",
      "########################################################################\r\n",
      "# End of output for job 2917.v-qsvr-1.devcloud-edge\r\n",
      "# Date: Thu Nov 14 01:26:03 PST 2019\r\n",
      "########################################################################\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_cpu = !qsub object_detection.sh -l nodes=1:idc001skl:tank-870:i5-6500te -F \"-r results/Core -d HETERO:CPU,GPU -f FP32 -i $VIDEO -n 4\" -N obj_det_cpu \n",
    "print(job_id_cpu[0]) \n",
    "if job_id_cpu:\n",
    "    progressIndicator('results/Core', 'pre_progress.txt', \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/Core', 'i_progress.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/Core', 'post_progress.txt', \"Rendering\", 0, 100)\n",
    "while True:\n",
    "    var=job_id_cpu[0].split(\".\")\n",
    "    file=\"obj_det_cpu.o\"+var[0]\n",
    "    if os.path.isfile(file): \n",
    "        ! cat $file\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Observe the performance time required to process each frame by Inference Engine. For this particular example, inference ran faster when prioritized for CPU as oppose to when GPU was the first priority.\n",
    "\n",
    " \n",
    "## Inference Engine classification sample\n",
    "\n",
    "\n",
    "Intel® Distribution of OpenVINO™ toolkit install folder (/opt/intel/openvino) includes various samples for developers to understand how Inference Engine APIs can be used. These samples have -pc flag implmented which shows per topology layer performance report. This will allow to see which layers are running on which hardware. We will run a very basic classification sample as an example in this section. We will provide car image as input to the classification sample. The output will be object labels with confidence numbers.\n",
    "\n",
    "### 1. First, get the classification model and convert that to IR using Model Optimizer\n",
    "\n",
    "For this example, we will use squeezenet model downloaded with the model downloader script while setting up the OS for the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading models ||################\n",
      "\n",
      "========== Downloading models/public/squeezenet1.1/squeezenet1.1.prototxt\n",
      "... 100%, 9 KB, 31560 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading models/public/squeezenet1.1/squeezenet1.1.caffemodel\n",
      "... 100%, 4834 KB, 27619 KB/s, 0 seconds passed\n",
      "\n",
      "################|| Post-processing ||################\n",
      "\n",
      "========== Replacing text in models/public/squeezenet1.1/squeezenet1.1.prototxt\n"
     ]
    }
   ],
   "source": [
    "! /opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name squeezenet1.1 -o models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/public/squeezenet1.1/squeezenet1.1.caffemodel\n",
      "\t- Path for generated IR: \t/home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/squeezenet/FP32/\n",
      "\t- IR output name: \tsqueezenet1.1\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Path to Python Caffe* parser generated from caffe.proto: \t/opt/intel/openvino/deployment_tools/model_optimizer/mo/front/caffe/proto\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/public/squeezenet1.1/squeezenet1.1.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \tDefault\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "Model Optimizer version: \t2019.3.0-375-g332562022\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: /home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/squeezenet/FP32/squeezenet1.1.xml\n",
      "[ SUCCESS ] BIN file: /home/u33131/13Nov-SVW-R3/hardware-heterogeneity/devcloud/python/models/squeezenet/FP32/squeezenet1.1.bin\n",
      "[ SUCCESS ] Total execution time: 2.66 seconds. \n"
     ]
    }
   ],
   "source": [
    "! /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model models/public/squeezenet1.1/squeezenet1.1.caffemodel -o models/squeezenet/FP32/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To display labels after classifictaion, you will need a labels file for the SqueezeNet* model. Get the available labels file from demo directory to your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /opt/intel/openvino/deployment_tools/demo/squeezenet1.1.labels models/squeezenet/FP32/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will us the [car_1.bmp](car_1.bmp) image to run our classification job as described in the next steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "### 2. Run classification sample with hetero plugin, prioritizing running on GPU first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting classification_job.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile classification_job.sh\n",
    "ME=`basename $0`\n",
    "\n",
    "DEVICE=$2\n",
    "\n",
    "# Object detection script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "# The output directory is the first argument of the bash script\n",
    "while getopts 'd:f:i:r:n:?' OPTION; do\n",
    "    case \"$OPTION\" in\n",
    "    d)\n",
    "        DEVICE=$OPTARG\n",
    "        echo \"$ME is using device $OPTARG\"\n",
    "      ;;\n",
    "\n",
    "    f)\n",
    "        FP_MODEL=$OPTARG\n",
    "        echo \"$ME is using floating point model $OPTARG\"\n",
    "      ;;\n",
    "\n",
    "    i)\n",
    "        INPUT_FILE=$OPTARG\n",
    "        echo \"$ME is using input file $OPTARG\"\n",
    "      ;;\n",
    "    r)\n",
    "        RESULTS_BASE=$OPTARG\n",
    "        echo \"$ME is using results base $OPTARG\"\n",
    "      ;;\n",
    "    n)\n",
    "        NUM_INFER_REQS=$OPTARG\n",
    "        echo \"$ME is running $OPTARG inference requests\"\n",
    "      ;;\n",
    "    esac  \n",
    "done\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "#NN_MODEL=\"mobilenet-ssd.xml\"\n",
    "RESULTS_PATH=\"${RESULTS_BASE}\"\n",
    "#mkdir -p $RESULTS_PATH\n",
    "echo \"$ME is using results path $RESULTS_PATH\"\n",
    "\n",
    "if [ \"$DEVICE\" == \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/opt/altera/aocl-pro-rte/aclrte-linux64/\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    source /opt/fpga_support_files/setup_env.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_bitstreams/2019R1_PL1_FP11_MobileNet_Clamp.aocx\n",
    "fi\n",
    "    \n",
    "# Running the object detection code\n",
    "#SAMPLEPATH=$PBS_O_WORKDIR\n",
    "python3 classification_sample.py                     -i car_1.bmp \\\n",
    "                                            -m models/squeezenet/FP32/squeezenet1.1.xml \\\n",
    "                                            -d $DEVICE \\\n",
    "                                            -pc  \n",
    "\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2991.v-qsvr-1.devcloud-edge\n",
      "\n",
      "########################################################################\n",
      "#      Date:           Thu Nov 14 04:05:20 PST 2019\n",
      "#    Job ID:           2991.v-qsvr-1.devcloud-edge\n",
      "#      User:           u33131\n",
      "# Resources:           neednodes=1:idc001skl:intel-hd-530,nodes=1:idc001skl:intel-hd-530,walltime=01:00:00\n",
      "########################################################################\n",
      "\n",
      "[setupvars.sh] OpenVINO environment initialized\n",
      "2991.v-qsvr-1.devcloud-edge.SC is using results path \n",
      "[ INFO ] Loading network files:\n",
      "\tmodels/squeezenet/FP32/squeezenet1.1.xml\n",
      "\tmodels/squeezenet/FP32/squeezenet1.1.bin\n",
      "[ INFO ] Preparing input blobs\n",
      "size is 1\n",
      "[ WARNING ] Image car_1.bmp is resized from (637, 749) to (227, 227)\n",
      "[ INFO ] Batch size is 1\n",
      "[ INFO ] Loading model to the plugin\n",
      "[ INFO ] Starting inference (1 iterations)\n",
      "[ INFO ] Processing output blob\n",
      "[ INFO ] Top 10 results: \n",
      "Image car_1.bmp\n",
      "\n",
      "899  0.2101037 label jug\n",
      "882  0.1619387 label vacuum cleaner\n",
      "438  0.0896358 label beaker\n",
      "804  0.0695650 label dispenser\n",
      "898  0.0602500 label bottle\n",
      "503  0.0573364 label shaker\n",
      "818  0.0302899 label spot\n",
      "505  0.0283235 label coffeepot\n",
      "604  0.0228082 label hourglass\n",
      "859  0.0183770 label toaster\n",
      "\n",
      "\n",
      "[ INFO ] Average running time of one iteration: 30.128955841064453 ms\n",
      "[ INFO ] total running time of inference: 30.128955841064453 ms\n",
      "[ INFO ] Throughput: 33.19066234074543 FPS\n",
      "\n",
      "\n",
      "performance counts:\n",
      "\n",
      "subgraph0: conv1                         EXECUTED        layerType: Convolution     realTime: 253   cpu:  11    execType:  convolution_gpu_bfyx_to_bfyx_f16\n",
      "TotalTime:           253     microseconds        \n",
      "subgraph0: conv10                        EXECUTED        layerType: Convolution     realTime: 830   cpu:  6     execType:  convolution_gpu_bfyx_f16\n",
      "TotalTime:           1083    microseconds        \n",
      "subgraph0: data                          EXECUTED        layerType: Input_layout    realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1083    microseconds        \n",
      "subgraph0: fire2/concat                  OPTIMIZED_OUT   layerType: Concat          realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1083    microseconds        \n",
      "subgraph0: fire2/expand1x1               EXECUTED        layerType: Convolution     realTime: 65    cpu:  7     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           1148    microseconds        \n",
      "subgraph0: fire2/expand3x3               EXECUTED        layerType: Convolution     realTime: 202   cpu:  6     execType:  convolution_gpu_bfyx_f16\n",
      "TotalTime:           1350    microseconds        \n",
      "subgraph0: fire2/relu_expand1x1          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1350    microseconds        \n",
      "subgraph0: fire2/relu_expand3x3          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1350    microseconds        \n",
      "subgraph0: fire2/relu_squeeze1x1         OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1350    microseconds        \n",
      "subgraph0: fire2/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 46    cpu:  6     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           1396    microseconds        \n",
      "subgraph0: fire3/concat                  OPTIMIZED_OUT   layerType: Concat          realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1396    microseconds        \n",
      "subgraph0: fire3/expand1x1               EXECUTED        layerType: Convolution     realTime: 65    cpu:  5     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           1461    microseconds        \n",
      "subgraph0: fire3/expand3x3               EXECUTED        layerType: Convolution     realTime: 204   cpu:  5     execType:  convolution_gpu_bfyx_f16\n",
      "TotalTime:           1665    microseconds        \n",
      "subgraph0: fire3/relu_expand1x1          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1665    microseconds        \n",
      "subgraph0: fire3/relu_expand3x3          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1665    microseconds        \n",
      "subgraph0: fire3/relu_squeeze1x1         OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1665    microseconds        \n",
      "subgraph0: fire3/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 83    cpu:  5     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           1748    microseconds        \n",
      "subgraph0: fire4/concat                  OPTIMIZED_OUT   layerType: Concat          realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1748    microseconds        \n",
      "subgraph0: fire4/expand1x1               EXECUTED        layerType: Convolution     realTime: 51    cpu:  5     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           1799    microseconds        \n",
      "subgraph0: fire4/expand3x3               EXECUTED        layerType: Convolution     realTime: 230   cpu:  5     execType:  convolution_gpu_bfyx_f16\n",
      "TotalTime:           2029    microseconds        \n",
      "subgraph0: fire4/relu_expand1x1          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2029    microseconds        \n",
      "subgraph0: fire4/relu_expand3x3          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2029    microseconds        \n",
      "subgraph0: fire4/relu_squeeze1x1         OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2029    microseconds        \n",
      "subgraph0: fire4/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 43    cpu:  5     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           2072    microseconds        \n",
      "subgraph0: fire5/concat                  OPTIMIZED_OUT   layerType: Concat          realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2072    microseconds        \n",
      "subgraph0: fire5/expand1x1               EXECUTED        layerType: Convolution     realTime: 52    cpu:  5     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           2124    microseconds        \n",
      "subgraph0: fire5/expand3x3               EXECUTED        layerType: Convolution     realTime: 231   cpu:  5     execType:  convolution_gpu_bfyx_f16\n",
      "TotalTime:           2355    microseconds        \n",
      "subgraph0: fire5/relu_expand1x1          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2355    microseconds        \n",
      "subgraph0: fire5/relu_expand3x3          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2355    microseconds        \n",
      "subgraph0: fire5/relu_squeeze1x1         OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2355    microseconds        \n",
      "subgraph0: fire5/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 71    cpu:  5     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           2426    microseconds        \n",
      "subgraph0: fire6/concat                  OPTIMIZED_OUT   layerType: Concat          realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2426    microseconds        \n",
      "subgraph0: fire6/expand1x1               EXECUTED        layerType: Convolution     realTime: 30    cpu:  5     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           2456    microseconds        \n",
      "subgraph0: fire6/expand3x3               EXECUTED        layerType: Convolution     realTime: 129   cpu:  6     execType:  convolution_gpu_bfyx_f16\n",
      "TotalTime:           2585    microseconds        \n",
      "subgraph0: fire6/relu_expand1x1          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2585    microseconds        \n",
      "subgraph0: fire6/relu_expand3x3          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2585    microseconds        \n",
      "subgraph0: fire6/relu_squeeze1x1         OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2585    microseconds        \n",
      "subgraph0: fire6/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 34    cpu:  5     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           2619    microseconds        \n",
      "subgraph0: fire7/concat                  OPTIMIZED_OUT   layerType: Concat          realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2619    microseconds        \n",
      "subgraph0: fire7/expand1x1               EXECUTED        layerType: Convolution     realTime: 29    cpu:  5     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           2648    microseconds        \n",
      "subgraph0: fire7/expand3x3               EXECUTED        layerType: Convolution     realTime: 129   cpu:  5     execType:  convolution_gpu_bfyx_f16\n",
      "TotalTime:           2777    microseconds        \n",
      "subgraph0: fire7/relu_expand1x1          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2777    microseconds        \n",
      "subgraph0: fire7/relu_expand3x3          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2777    microseconds        \n",
      "subgraph0: fire7/relu_squeeze1x1         OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2777    microseconds        \n",
      "subgraph0: fire7/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 45    cpu:  5     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           2822    microseconds        \n",
      "subgraph0: fire8/concat                  OPTIMIZED_OUT   layerType: Concat          realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2822    microseconds        \n",
      "subgraph0: fire8/expand1x1               EXECUTED        layerType: Convolution     realTime: 46    cpu:  6     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           2868    microseconds        \n",
      "subgraph0: fire8/expand3x3               EXECUTED        layerType: Convolution     realTime: 239   cpu:  5     execType:  convolution_gpu_bfyx_f16\n",
      "TotalTime:           3107    microseconds        \n",
      "subgraph0: fire8/relu_expand1x1          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           3107    microseconds        \n",
      "subgraph0: fire8/relu_expand3x3          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           3107    microseconds        \n",
      "subgraph0: fire8/relu_squeeze1x1         OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           3107    microseconds        \n",
      "subgraph0: fire8/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 53    cpu:  5     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           3160    microseconds        \n",
      "subgraph0: fire9/concat                  OPTIMIZED_OUT   layerType: Concat          realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           3160    microseconds        \n",
      "subgraph0: fire9/expand1x1               EXECUTED        layerType: Convolution     realTime: 52    cpu:  5     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           3212    microseconds        \n",
      "subgraph0: fire9/expand3x3               EXECUTED        layerType: Convolution     realTime: 245   cpu:  5     execType:  convolution_gpu_bfyx_f16\n",
      "TotalTime:           3457    microseconds        \n",
      "subgraph0: fire9/relu_expand1x1          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           3457    microseconds        \n",
      "subgraph0: fire9/relu_expand3x3          OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           3457    microseconds        \n",
      "subgraph0: fire9/relu_squeeze1x1         OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           3457    microseconds        \n",
      "subgraph0: fire9/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 71    cpu:  5     execType:  convolution_gpu_bfyx_f16_1x1\n",
      "TotalTime:           3528    microseconds        \n",
      "subgraph0: input:data_cldnn_input_preprocess OPTIMIZED_OUT   layerType: reorder         realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           3528    microseconds        \n",
      "subgraph0: pool1                         EXECUTED        layerType: Pooling         realTime: 97    cpu:  7     execType:  pooling_gpu_blocked\n",
      "TotalTime:           3625    microseconds        \n",
      "subgraph0: pool10                        EXECUTED        layerType: Pooling         realTime: 21    cpu:  5     execType:  pooling_gpu_blocked\n",
      "TotalTime:           3646    microseconds        \n",
      "subgraph0: pool10_reorder_0              EXECUTED        layerType: reorder         realTime: 3     cpu:  5     execType:  reorder_data_fast_b1\n",
      "TotalTime:           3649    microseconds        \n",
      "subgraph0: pool3                         EXECUTED        layerType: Pooling         realTime: 52    cpu:  5     execType:  pooling_gpu_blocked\n",
      "TotalTime:           3701    microseconds        \n",
      "subgraph0: pool5                         EXECUTED        layerType: Pooling         realTime: 40    cpu:  5     execType:  pooling_gpu_blocked\n",
      "TotalTime:           3741    microseconds        \n",
      "subgraph0: prob                          EXECUTED        layerType: SoftMax         realTime: 15    cpu:  6     execType:  softmax_gpu_bf \n",
      "TotalTime:           3756    microseconds        \n",
      "subgraph0: prob_cldnn_output_postprocess EXECUTED        layerType: reorder         realTime: 3     cpu:  5     execType:  reorder_data_fast_b1\n",
      "TotalTime:           3759    microseconds        \n",
      "subgraph0: relu_conv1                    OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           3759    microseconds        \n",
      "subgraph0: relu_conv10                   OPTIMIZED_OUT   layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           3759    microseconds        \n",
      "[ INFO ] Execution successful\n",
      "\n",
      "########################################################################\n",
      "# End of output for job 2991.v-qsvr-1.devcloud-edge\n",
      "# Date: Thu Nov 14 04:05:34 PST 2019\n",
      "########################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub classification_job.sh -l nodes=1:idc001skl:intel-hd-530 -F \"results/GPU HETERO:GPU,CPU FP32\" -N obj_det_gpu \n",
    "print(job_id_gpu[0]) \n",
    "while True:\n",
    "    var=job_id_gpu[0].split(\".\")\n",
    "    file=\"obj_det_gpu.o\"+var[0]\n",
    "    if os.path.isfile(file): \n",
    "        ! cat $file\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After the execution, You should get the performance counters output as in the screenshot below:-\n",
    "\n",
    "\n",
    "<img src='gpu.png'>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "### 3. Now, run with CPU first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2992.v-qsvr-1.devcloud-edge\n",
      "\n",
      "########################################################################\n",
      "#      Date:           Thu Nov 14 04:06:22 PST 2019\n",
      "#    Job ID:           2992.v-qsvr-1.devcloud-edge\n",
      "#      User:           u33131\n",
      "# Resources:           neednodes=1:idc001skl:tank-870:i5-6500te,nodes=1:idc001skl:tank-870:i5-6500te,walltime=01:00:00\n",
      "########################################################################\n",
      "\n",
      "[setupvars.sh] OpenVINO environment initialized\n",
      "2992.v-qsvr-1.devcloud-edge.SC is using results path \n",
      "[ INFO ] Loading network files:\n",
      "\tmodels/squeezenet/FP32/squeezenet1.1.xml\n",
      "\tmodels/squeezenet/FP32/squeezenet1.1.bin\n",
      "[ INFO ] Preparing input blobs\n",
      "size is 1\n",
      "[ WARNING ] Image car_1.bmp is resized from (637, 749) to (227, 227)\n",
      "[ INFO ] Batch size is 1\n",
      "[ INFO ] Loading model to the plugin\n",
      "[ INFO ] Starting inference (1 iterations)\n",
      "[ INFO ] Processing output blob\n",
      "[ INFO ] Top 10 results: \n",
      "Image car_1.bmp\n",
      "\n",
      "899  0.2101035 label jug\n",
      "882  0.1619391 label vacuum cleaner\n",
      "438  0.0896353 label beaker\n",
      "804  0.0695652 label dispenser\n",
      "898  0.0602502 label bottle\n",
      "503  0.0573365 label shaker\n",
      "818  0.0302898 label spot\n",
      "505  0.0283233 label coffeepot\n",
      "604  0.0228083 label hourglass\n",
      "859  0.0183770 label toaster\n",
      "\n",
      "\n",
      "[ INFO ] Average running time of one iteration: 32.25064277648926 ms\n",
      "[ INFO ] total running time of inference: 32.25064277648926 ms\n",
      "[ INFO ] Throughput: 31.007133933125846 FPS\n",
      "\n",
      "\n",
      "performance counts:\n",
      "\n",
      "subgraph0: conv1                         EXECUTED        layerType: Convolution     realTime: 419   cpu:  419   execType:  jit_avx2_FP32  \n",
      "TotalTime:           419     microseconds        \n",
      "subgraph0: conv10                        EXECUTED        layerType: Convolution     realTime: 602   cpu:  602   execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           1021    microseconds        \n",
      "subgraph0: fire2/concat                  EXECUTED        layerType: Concat          realTime: 2     cpu:  2     execType:  unknown_FP32   \n",
      "TotalTime:           1023    microseconds        \n",
      "subgraph0: fire2/expand1x1               EXECUTED        layerType: Convolution     realTime: 24    cpu:  24    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           1047    microseconds        \n",
      "subgraph0: fire2/expand3x3               EXECUTED        layerType: Convolution     realTime: 162   cpu:  162   execType:  jit_avx2_FP32  \n",
      "TotalTime:           1209    microseconds        \n",
      "subgraph0: fire2/relu_expand1x1          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1209    microseconds        \n",
      "subgraph0: fire2/relu_expand3x3          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1209    microseconds        \n",
      "subgraph0: fire2/relu_squeeze1x1         NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1209    microseconds        \n",
      "subgraph0: fire2/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 28    cpu:  28    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           1237    microseconds        \n",
      "subgraph0: fire3/concat                  EXECUTED        layerType: Concat          realTime: 1     cpu:  1     execType:  unknown_FP32   \n",
      "TotalTime:           1238    microseconds        \n",
      "subgraph0: fire3/expand1x1               EXECUTED        layerType: Convolution     realTime: 23    cpu:  23    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           1261    microseconds        \n",
      "subgraph0: fire3/expand3x3               EXECUTED        layerType: Convolution     realTime: 162   cpu:  162   execType:  jit_avx2_FP32  \n",
      "TotalTime:           1423    microseconds        \n",
      "subgraph0: fire3/relu_expand1x1          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1423    microseconds        \n",
      "subgraph0: fire3/relu_expand3x3          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1423    microseconds        \n",
      "subgraph0: fire3/relu_squeeze1x1         NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1423    microseconds        \n",
      "subgraph0: fire3/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 44    cpu:  44    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           1467    microseconds        \n",
      "subgraph0: fire4/concat                  EXECUTED        layerType: Concat          realTime: 1     cpu:  1     execType:  unknown_FP32   \n",
      "TotalTime:           1468    microseconds        \n",
      "subgraph0: fire4/expand1x1               EXECUTED        layerType: Convolution     realTime: 22    cpu:  22    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           1490    microseconds        \n",
      "subgraph0: fire4/expand3x3               EXECUTED        layerType: Convolution     realTime: 165   cpu:  165   execType:  jit_avx2_FP32  \n",
      "TotalTime:           1655    microseconds        \n",
      "subgraph0: fire4/relu_expand1x1          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1655    microseconds        \n",
      "subgraph0: fire4/relu_expand3x3          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1655    microseconds        \n",
      "subgraph0: fire4/relu_squeeze1x1         NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1655    microseconds        \n",
      "subgraph0: fire4/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 24    cpu:  24    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           1679    microseconds        \n",
      "subgraph0: fire5/concat                  EXECUTED        layerType: Concat          realTime: 1     cpu:  1     execType:  unknown_FP32   \n",
      "TotalTime:           1680    microseconds        \n",
      "subgraph0: fire5/expand1x1               EXECUTED        layerType: Convolution     realTime: 22    cpu:  22    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           1702    microseconds        \n",
      "subgraph0: fire5/expand3x3               EXECUTED        layerType: Convolution     realTime: 164   cpu:  164   execType:  jit_avx2_FP32  \n",
      "TotalTime:           1866    microseconds        \n",
      "subgraph0: fire5/relu_expand1x1          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1866    microseconds        \n",
      "subgraph0: fire5/relu_expand3x3          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1866    microseconds        \n",
      "subgraph0: fire5/relu_squeeze1x1         NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           1866    microseconds        \n",
      "subgraph0: fire5/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 44    cpu:  44    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           1910    microseconds        \n",
      "subgraph0: fire6/concat                  EXECUTED        layerType: Concat          realTime: 1     cpu:  1     execType:  unknown_FP32   \n",
      "TotalTime:           1911    microseconds        \n",
      "subgraph0: fire6/expand1x1               EXECUTED        layerType: Convolution     realTime: 14    cpu:  14    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           1925    microseconds        \n",
      "subgraph0: fire6/expand3x3               EXECUTED        layerType: Convolution     realTime: 102   cpu:  102   execType:  jit_avx2_FP32  \n",
      "TotalTime:           2027    microseconds        \n",
      "subgraph0: fire6/relu_expand1x1          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2027    microseconds        \n",
      "subgraph0: fire6/relu_expand3x3          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2027    microseconds        \n",
      "subgraph0: fire6/relu_squeeze1x1         NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2027    microseconds        \n",
      "subgraph0: fire6/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 18    cpu:  18    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           2045    microseconds        \n",
      "subgraph0: fire7/concat                  EXECUTED        layerType: Concat          realTime: 1     cpu:  1     execType:  unknown_FP32   \n",
      "TotalTime:           2046    microseconds        \n",
      "subgraph0: fire7/expand1x1               EXECUTED        layerType: Convolution     realTime: 14    cpu:  14    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           2060    microseconds        \n",
      "subgraph0: fire7/expand3x3               EXECUTED        layerType: Convolution     realTime: 117   cpu:  117   execType:  jit_avx2_FP32  \n",
      "TotalTime:           2177    microseconds        \n",
      "subgraph0: fire7/relu_expand1x1          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2177    microseconds        \n",
      "subgraph0: fire7/relu_expand3x3          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2177    microseconds        \n",
      "subgraph0: fire7/relu_squeeze1x1         NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2177    microseconds        \n",
      "subgraph0: fire7/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 25    cpu:  25    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           2202    microseconds        \n",
      "subgraph0: fire8/concat                  EXECUTED        layerType: Concat          realTime: 1     cpu:  1     execType:  unknown_FP32   \n",
      "TotalTime:           2203    microseconds        \n",
      "subgraph0: fire8/expand1x1               EXECUTED        layerType: Convolution     realTime: 23    cpu:  23    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           2226    microseconds        \n",
      "subgraph0: fire8/expand3x3               EXECUTED        layerType: Convolution     realTime: 181   cpu:  181   execType:  jit_avx2_FP32  \n",
      "TotalTime:           2407    microseconds        \n",
      "subgraph0: fire8/relu_expand1x1          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2407    microseconds        \n",
      "subgraph0: fire8/relu_expand3x3          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2407    microseconds        \n",
      "subgraph0: fire8/relu_squeeze1x1         NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2407    microseconds        \n",
      "subgraph0: fire8/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 33    cpu:  33    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           2440    microseconds        \n",
      "subgraph0: fire9/concat                  NOT_RUN         layerType: Concat          realTime: 0     cpu:  0     execType:  unknown_FP32   \n",
      "TotalTime:           2440    microseconds        \n",
      "subgraph0: fire9/expand1x1               EXECUTED        layerType: Convolution     realTime: 23    cpu:  23    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           2463    microseconds        \n",
      "subgraph0: fire9/expand3x3               EXECUTED        layerType: Convolution     realTime: 178   cpu:  178   execType:  jit_avx2_FP32  \n",
      "TotalTime:           2641    microseconds        \n",
      "subgraph0: fire9/relu_expand1x1          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2641    microseconds        \n",
      "subgraph0: fire9/relu_expand3x3          NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2641    microseconds        \n",
      "subgraph0: fire9/relu_squeeze1x1         NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           2641    microseconds        \n",
      "subgraph0: fire9/squeeze1x1              EXECUTED        layerType: Convolution     realTime: 43    cpu:  43    execType:  jit_avx2_1x1_FP32\n",
      "TotalTime:           2684    microseconds        \n",
      "subgraph0: out_prob                      NOT_RUN         layerType: Output          realTime: 0     cpu:  0     execType:  unknown_FP32   \n",
      "TotalTime:           2684    microseconds        \n",
      "subgraph0: pool1                         EXECUTED        layerType: Pooling         realTime: 136   cpu:  136   execType:  jit_avx_FP32   \n",
      "TotalTime:           2820    microseconds        \n",
      "subgraph0: pool10                        EXECUTED        layerType: Pooling         realTime: 12    cpu:  12    execType:  jit_avx_FP32   \n",
      "TotalTime:           2832    microseconds        \n",
      "subgraph0: pool3                         EXECUTED        layerType: Pooling         realTime: 29    cpu:  29    execType:  jit_avx_FP32   \n",
      "TotalTime:           2861    microseconds        \n",
      "subgraph0: pool5                         EXECUTED        layerType: Pooling         realTime: 18    cpu:  18    execType:  jit_avx_FP32   \n",
      "TotalTime:           2879    microseconds        \n",
      "subgraph0: prob                          EXECUTED        layerType: SoftMax         realTime: 14349 cpu:  14349 execType:  ref_any_FP32   \n",
      "TotalTime:           17228   microseconds        \n",
      "subgraph0: prob_nChw8c_nchw_out_prob     EXECUTED        layerType: Reorder         realTime: 4     cpu:  4     execType:  jit_uni_FP32   \n",
      "TotalTime:           17232   microseconds        \n",
      "subgraph0: relu_conv1                    NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           17232   microseconds        \n",
      "subgraph0: relu_conv10                   NOT_RUN         layerType: ReLU            realTime: 0     cpu:  0     execType:  undef          \n",
      "TotalTime:           17232   microseconds        \n",
      "[ INFO ] Execution successful\n",
      "\n",
      "########################################################################\n",
      "# End of output for job 2992.v-qsvr-1.devcloud-edge\n",
      "# Date: Thu Nov 14 04:06:31 PST 2019\n",
      "########################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_cpu = !qsub classification_job.sh -l nodes=1:idc001skl:tank-870:i5-6500te  -F \"results/GPU HETERO:CPU,GPU FP32\" -N obj_det_cpu\n",
    "print(job_id_cpu[0]) \n",
    "while True:\n",
    "    var=job_id_cpu[0].split(\".\")\n",
    "    file=\"obj_det_cpu.o\"+var[0]\n",
    "    if os.path.isfile(file): \n",
    "        ! cat $file\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After the execution, You should get the performance counters output as in the screenshot below:-\n",
    "\n",
    "\n",
    "<img src='cpu.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
